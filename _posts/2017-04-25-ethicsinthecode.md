---
layout: post
title:  "Confused Ethics"
date:   2017-04-25 00:16:45
categories:  coding, ethics
comments: true
tags: ethics
---

Most organized professions have an ethical code of some sort. Sometimes that takes the form of an historic and partly philosophical vow such as the Hippocratic Oath, and sometimes it's much more technical like the <a href='https://www.nspe.org/resources/ethics/code-ethics'> NSPE Code of Ethics For Engineers.</a> We, as software developers, have no counterpart to these codes, either philosophical nor technical.

I understand why that might be. At least I think I do. Software development is a young discipline, and it's the wild west in a lot of ways. Software development is still figuring out its identity, and still sifting through much of what it will be in the world. There is very little about it that is codified, and I don't think that's entirely a bad thing.

Software development is obviously different to a discipline like medicine or engineering (I won't be referring to us as engineers in the post, because I think it's easier to remain with that distinction). If a doctor gets it wrong, a person can die. If an engineer gets it wrong, a building can collapse. The ramifications are immediately and fatally catastrophic. Software isn't generally going to cause quite as spectacular a failure as those other professions, but it isn't without massive dangers to society.

Think about just one of the current hotspot debates in this arena, backdoors in personal devices which can be accessed by government entities. Let's set aside the thorny argument about whether or not it's ethical for an agency such as the NSA to have access to those devices for a moment (though that's certainly a worthwhile debate). Let's think about China, or Syria, or Iran, or any number of other nations. If those backdoors exist, if governments have leverage with the makers of tech to create those backdoors, why would we assume that only one nation would have access to them? China is now (by far) the largest market in the world for new technology. If the price of access to markets is to allow a government with a human rights record such as China's to have access to personal data on a never before imaginable level, would those tech firms (who are ultimately beholden to shareholders) agree to that? Who is culpable if that technology is used for evil? Is it the company itself? Is it the government of China? Is it the software team who built that backdoor? I don't really know.

The problem is that we don't have any reference to turn to when it comes to these kinds of issues. The power of what we build has far outpaced the maturity of our standards. When these debates do happen, they tend to be focused on technical problems, such as data security. But I think that misses the forest for the trees. What we do has enormous implications for the world, maybe not what you or I specifically do, but as an aggregate. I don't have any answers, but I do think it's a worthwhile conversation to start. 
